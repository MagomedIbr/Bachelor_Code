@article{article,
author = {Pérez-Córdoba, José and Martín-Doñas, Juan and Gomez, Angel and Gomez-Alanis, Alejandro and Gonzalez Lopez, Jose},
year = {2020},
month = {09},
pages = {},
title = {Silent Speech Interfaces for Speech Restoration: A Review}
}
@phdthesis{herff2016speech,
  school={University of Bremen},
  title={Speech Processes for Brain-Computer Interfaces},
  year={2016},
  supervisor={Schultz, Tanja and Krusienski, Dean J},
  url={https://www.csl.uni-bremen.de/cms/images/documents/publications/Herff2016Diss.pdf},
  author={Herff, Christian}
}


@phdthesis{janke2016emg,
  school={Karlsruher Institut für Technologie},
  title={EMG-to-Speech: Direct Generation of Speech from Facial Electromyographic Signals},
  year={2016},
  supervisor={Schultz, Tanja},
  author={Janke, Matthias},
  url={https://www.csl.uni-bremen.de/cms/images/documents/publications/janke_thesis.pdf},
}

@misc{StatBund, 
title={$Statistik der schwerbehinderten Menschen 2019 (Aufgerufen am 16.9.2020 https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Gesundheit/Behinderte-Menschen/_inhalt.html$\#sprg233848)},
 url={$https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Gesundheit/Behinderte-Menschen/_inhalt.html#sprg233848g$},
publisher={Statistisches Bundesamt (Destatis), 2020}
}




@inproceedings{schultz_embc_2019,
author = {Schultz, Tanja and Angrick, Miguel and Diener, Lorenz and Küster, Dennis and Meier, Moritz and Krusienski, Dean and Herff, Christian and Brumberg, Jonathan},
booktitle={2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
title={Towards Restoration of Articulatory Movements: Functional Electrical Stimulation of Orofacial Muscles},
year={2019},
volume={},
number={},
pages={3111-3114},
keywords={brain;muscle;neuromuscular stimulation;speech coding;speech synthesis;decoding speech-related brain activity;physical speech production;decoded speech-related brain activity;eventual orofacial stimulation;functional electrical stimulation;synthesized speech generation;physical speech restoration;electrical stimulation;orofacial muscles stimulation;acoustic production;articulatory movement restoration;Muscles;Production;Electromyography;Spectrogram;Correlation;Electrodes;Brain},
doi={10.1109/EMBC.2019.8857670},
ISSN={1557-170X},
month={July},
abstract={Millions of individuals suffer from impairments that significantly disrupt or completely eliminate their ability to speak. An ideal intervention would restore one's natural ability to physically produce speech. Recent progress has been made in decoding speech-related brain activity to generate synthesized speech. Our vision is to extend these recent advances toward the goal of restoring physical speech production using decoded speech-related brain activity to modulate the electrical stimulation of the orofacial musculature involved in speech. In this pilot study we take a step toward this vision by investigating the feasibility of stimulating orofacial muscles during vocalization in order to alter acoustic production. The results of our study provide necessary foundation for eventual orofacial stimulation controlled directly from decoded speech-related brain activity.},
url={https://www.csl.uni-bremen.de/cms/images/documents/publications/schultz_fes_embc2019.pdf},
}


@article{Phinyo2012-FT,
  author =    {Angkoon Phinyomark},
  title =     {Feature reduction and selection for EMG signal classification},
  journal =   {Expert Systems with Applications},
  volume =    {39}, 
  number =    {8},
  pages =     { 7420-7431}, 
  year =      {2012}
}

@ARTICLE{8114358,
  author={T. {Schultz} and M. {Wand} and T. {Hueber} and D. J. {Krusienski} and C. {Herff} and J. S. {Brumberg}},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Biosignal-Based Spoken Communication: A Survey}, 
  year={2017},
  volume={25},
  number={12},
  pages={2257-2271},}

@inproceedings{diener2020cslemgarray,
    title={{CSL-EMG\_Array: An Open Access Corpus for EMG-to-Speech Conversion}},
    author={Diener, Lorenz and  Roustay Vishkasougheh, Mehrdad and Schultz, Tanja},
    booktitle={{INTERSPEECH} 2020 - 21st Annual Conference of the International Speech Communication Association},
    year={2020 (to appear)},
    url={https://www.csl.uni-bremen.de/cms/images/documents/publications/Diener_IS2020_CSLEMGArray.pdf},
    abstract={We present a new open access corpus for the training and evaluation of EMG-to-Speech conversion systems based on array electromyographic recordings. The corpus is recorded with a recording paradigm closely mirroring realistic EMG-to-Speech usage scenarios, and includes evaluation data recorded from both audible as well as silent speech. The corpus consists of 9.5 hours of data, split into 12 sessions recorded from 8 speakers. Based on this corpus, we present initial benchmark results with a realistic online EMG-to-Speech conversion use case, both for the audible and silent speech subsets. We also present a method for drastically improving EMG-to-Speech system stability and performance in the presence of time-related artifacts.},
}

@inproceedings{WaJaSch2014-IS,
  year={2014},
  title={The EMG-UKA Corpus for Electromyographic Speech Processing},
  note={Interspeech 2014},
  booktitle={The 15th Annual Conference of the International Speech Communication Association, Singapore},
  url={https://www.csl.uni-bremen.de/cms/images/documents/publications/WandJankeSchultz_IS14_EMG-UKA-Corpus.pdf},
  abstract={This article gives an overview of the EMG-UKA corpus, a corpus of electromyographic (EMG) recordings of articulatory activity enabling speech processing (in particular speech recognition and synthesis) based on EMG signals, with the purpose of building Silent Speech interfaces. Data is available in multiple speaking modes, namely audibly spoken, whispered, and silently articulated speech. Besides the EMG data, synchronous acoustic data was additionally recorded to serve as a reference. The corpus comprises 63 recorded sessions from 8 speakers, the total amount of data is 7:32 hours. A trial subset, consisting of 1:52 hours of data, is freely available for download.},
  author={Wand, Michael and Janke, Matthias and Schultz, Tanja}
}


@inproceedings{KonEMG2006-ATO,
  year={2006},
  title={A Practical Introduction  to Kinesiological Electromyography},
  note={EMG book},
  booktitle={The ABC of EMG},
  abstract={.},
  author={Peter Konrad }
}

@inproceedings{ShwDav2014-ATO,
  year={2014},
  title={Understanding Machine Learning: From Theory to Algorithms },
  note={ML Book},
  booktitle={Understanding Machine Learning: From Theory to Algorithms },
  abstract={.},
  author={Shai Shalev-Shwartz and Shai Ben-David }
}

@inproceedings{MorSteMer2004-ATO,
  year={2004},
  title={BASIC PHYSIOLOGY AND BIOPHYSICS OF EMG  SIGNAL GENERATION},
  note={EMG  Book},
  booktitle={ELECTROMYOGRAPHY Physiology, Engineering,and NoninvasiveApplications },
  abstract={.},
  author={T. Moritani, D. Stegeman, R. Merletti }
}

@Inbook{Docio-Fernandez2015,
author="Docio-Fernandez, Laura
and Garc{\'i}a Mateo, Carmen",
editor="Li, Stan Z.
and Jain, Anil K.",
title="Speech Production",
bookTitle="Encyclopedia of Biometrics",
year="2015",
publisher="Springer US",
address="Boston, MA",
pages="1493--1498",
isbn="978-1-4899-7488-4",
doi = {10.1007/978-0-387-68772-8_18},
url="https://doi.org/10.1007/978-1-4899-7488-4_199"
}

@misc{AidenLee2019-GIT,
  author = {Aiden Lee},
  title = {Machine Learning - EMG Project for Prosthesis Patients},
  year = {2019},
  publisher = {Gitlab},
  journal = {Gitlab repository},
  howpublished = {\url{https://gitlab.com/AidenLee94/machine-learning-emg-project-patients}},
  projektID = {12173698},
  commitSHA = {96fdb292}
}

@misc{AUDEEP-GIT,
  author = {S Amiriparian, M Freitag, N Cummins and B Schuller},
  title = {},
  year = {2020},
  publisher = {GitHub},
  journal = {Sequence to sequence autoencoders for unsupervised representation learning from audio, Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop, pp. 17-21, 2017},
  howpublished = {\url{https://github.com/auDeep/auDeep}},
  projektID = {},
  commitSHA = {de29c51}
}



@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{Avan2001-BOX,
  author =    {A. van Boxtel},
  title =     { Optimal signal bandwidth for the recording of surface EMG activity of facial, jaw, oral, and neck muscles. Psychophysiology},
  journal =   {Pubmeds},
  url="https://pubmed.ncbi.nlm.nih.gov/11321618/",
  volume =    {38}, 
  pages =     { 22-34}, 
  year =      {2001}
}

@misc{ISCA2020-SP, title={Interspeech Website https://www.isca-speech.org/iscaweb/index.php/online-archive}, url={https://www.isca-speech.org/iscaweb/index.php/online-archive}, journal={}, publisher={Interspeech}
}

@INPROCEEDINGS{6415781,
  author={Y. {Deng} and G. {Colby} and J. T. {Heaton} and G. S. {Meltzner}},
  booktitle={MILCOM 2012 - 2012 IEEE Military Communications Conference}, 
  title={Signal processing advances for the MUTE sEMG-based silent speech recognition system}, 
  year={2012},
  volume={},
  number={},
  pages={1-6},
  abstract={Military speech communication often needs to be conducted in very high noise environments. In addition, there are scenarios, such as special-ops missions, for which it is beneficial to have covert voice communications. To enable both capabilities, we have developed the MUTE (Mouthed-speech Understanding and Transcription Engine) system, which bypasses the limitations of traditional acoustic speech communication by measuring and interpreting muscle activity of the facial and neck musculature involved in silent speech production. This article details our recent progress on automatic surface electromyography (sEMG) speech activity detection, feature parameterization, multi-task sEMG corpus development, context dependent sub-word sEMG modeling, discriminative phoneme model training, and flexible vocabulary continuous sEMG silent speech recognition. Our current system achieved recognition accuracy at developable levels for a pre-defined special ops task. We further propose research directions in adaptive sEMG feature parameterization and data driven decision question generation for context-dependent sEMG phoneme modeling.},
  keywords={acoustic signal processing;military communication;speech recognition;signal processing advance;MUTE sEMG based silent speech recognition system;military speech communication;special-ops mission;covert voice communication;acoustic speech communication;neck musculature;silent speech production;automatic surface electromyography;speech activity detection;multitask sEMG corpus development;context dependent subword sEMG modeling;discriminative phoneme model training;flexible vocabulary continuous sEMG silent speech recognition;predefined special ops task;data driven decision question generation;context dependent sEMG phoneme modeling;Speech recognition;Speech;Sensors;Hidden Markov models;Training;Vocabulary;Accuracy;sEMG speech detection;sEMG speech recognition;silent speech communication;sub-word sEMG model;speaker adaptive sEMG feature extraction},
  doi={10.1109/MILCOM.2012.6415781},
  ISSN={2155-7586},
  month={Oct},}

@ARTICLE{8114359,
  author={M. {Janke} and L. {Diener}},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={EMG-to-Speech: Direct Generation of Speech From Facial Electromyographic Signals}, 
  year={2017},
  volume={25},
  number={12},
  pages={2375-2385},}

@misc{Spektrum, title={Bild über den Aufbau von einem Skelettmuskel Aufgerufen am 17.9.2020(https://www.spektrum.de/lexika/images/bio)}, url={https://www.spektrum.de/lexika/images/bio/f7f5602_w.jpg}, journal={spektrum.de}, publisher={spektrum}}

@misc{kenhub, title={Bild über die Muskeln im Menschlichen Gesicht Aufgerufen am 17.9.2020( Aufgerufen am 
https://www.kenhub.com/de/library/anatomie/die-mimische-muskulatur)}, url={https://www.spektrum.de/lexika/images/bio/f7f5602_w.jpg}, journal={spektrum.de}, publisher={spektrum}}


@misc{trainingsworld, title={Aufbau von Muskeln aufgerufen am 22.9.2020(
https://www.trainingsworld.com/training/muskelaufbau/physiologie-aufbau-funktion-skelettmuskulatur-2379200)}, url={https://www.trainingsworld.com/training/muskelaufbau/physiologie-aufbau-funktion-skelettmuskulatur-2379200},publisher={Angi Peukert}}

@misc{Wikimedia-Common, title={Datensatz für KNN Aufgerufen am 12.9.2020(https://commons.wikimedia.org/wiki/File:Map1NN.png)}, url={https://commons.wikimedia.org/wiki/File:Map1NN.png},publisher={Agor153}}

@misc{Wikimedia-Common2, title={Bild einer datensatzes nach einer Anpassung von einem KNN Aufgerufen am 12.9.2020(https://commons.wikimedia.org/wiki/File:Data3classes.png)}, url={https://commons.wikimedia.org/wiki/File:Data3classes.png},publisher={Agor153}}

@misc{Wikimedia-Common3, title={Bild der LDA Aufgerufen am 12.9.2020(
https://upload.wikimedia.org/wikipedia/de/a/a0/Diskriminanzfunktion.png)}, url={
https://upload.wikimedia.org/wikipedia/de/a/a0/Diskriminanzfunktion.png},publisher={Agor153}}


@misc{Wikimedia-Common4, title={Positionen des Pharynx und Larynx Aufgerufen am 17.9.2020($https://commons.wikimedia.org/wiki/File=Throat_Diagram.png$}, url={$https://commons.wikimedia.org/wiki/File:Throat_Diagram.png$}}

@inproceedings{janke2011investigations,
  title={Investigations on Speaking Mode Discrepancies in EMG-based Speech Recognition},
  year={2011},
  note={Interspeech 2011},
  booktitle={12th Annual Conference of the International Speech Communication Association},
  url={https://www.csl.uni-bremen.de/cms/images/documents/publications/WandJankeSchultz_Interspeech2011.pdf},
  abstract={In this paper we present our recent study on the impact of speaking mode variabilities on speech recognition by surface electromyography (EMG). Surface electromyography captures the electric potentials of the human articulatory muscles, which enables a user to communicate naturally without making any audible sound. Our previous experiments have shown that the EMG signal varies greatly between different speaking modes, like audibly uttered speech and silently articulated speech. In this study we extend our previous research and quantify the impact of different speaking modes by investigating the amount of mode-specific leaves in phonetic decision trees. We show that this measure correlates highly with discrepancies in the spectral energy of the EMG signal, as well as with differences in the performance of a recognizer on different speaking modes. We furthermore present how EMG signal adaptation by spectral mapping decreases the effect of the speaking mode.},
  keywords={EMG, EMG-based speech recognition, Silent
Speech Interfaces, phonetic decision tree},
  author={Janke, Matthias and Wand, Michael and Schultz, Tanja}
}


@article{Tanja_Interspeechkeynote_2019,
	address = {Graz, Austria},
	title = {Biosignal Processing for Human-Machine Interaction. In: Keynote presented at Interspeech 2019, September 2019 Graz, Austria},
    year={2019},
    month={09},
    day={16},
	author = {Schultz, Tanja},
	url={https://www.csl.uni-bremen.de/cms/images/documents/publications/Interspeech2019-KeynoteTanjaSchultz.pdf},
	abstract={Tanja Schultz held a keynote on "Biosignal Processing for Human-Machine Interaction" at Interspeech 2019, the largest conference on the science and technology of spoken language processing, and flagship of ISCA. Talk on Youtube (https://www.youtube.com/watch?v=XzkQnzP7yPA) Slides for download}
}

@book{10.5555/3133359,
author = {VanderPlas, Jake},
title = {Python Data Science Handbook: Essential Tools for Working with Data},
year = {2016},
isbn = {1491912057},
publisher = {O'Reilly Media, Inc.},
edition = {1st},
abstract = {For many researchers, Python is a first-class tool mainly because of its libraries for storing, manipulating, and gaining insight from data. Several resources exist for individual pieces of this data science stack, but only with the Python Data Science Handbook do you get them all IPython, NumPy, Pandas, Matplotlib, Scikit-Learn, and other related tools. Working scientists and data crunchers familiar with reading and writing Python code will find this comprehensive desk reference ideal for tackling day-to-day issues: manipulating, transforming, and cleaning data; visualizing different types of data; and using data to build statistical or machine learning models. Quite simply, this is the must-have reference for scientific computing in Python. With this handbook, youll learn how to use:IPython and Jupyter: provide computational environments for data scientists using PythonNumPy: includes the ndarray for efficient storage and manipulation of dense data arrays in PythonPandas: features the DataFrame for efficient storage and manipulation of labeled/columnar data in PythonMatplotlib: includes capabilities for a flexible range of data visualizations in PythonScikit-Learn: for efficient and clean Python implementations of the most important and established machine learning algorithms}
}


@inproceedings{KonEMG2001-ATO,
  year={2001},
  title={Eine praxisorientierte Einführung
in die kinesiologische Elektromyographie},
  note={EMG Fibel},
  booktitle={EMG-FIBEL},
  abstract={.},
  author={Peter Konrad }
}

@inproceedings{diener2020towards,
    title={Towards Silent Paralinguistics: Deriving Speaking Mode and Speaker ID from Electromyographic Signals},
    author={Diener, Lorenz and Amiriparian, Shahin and Botelho, Catarina and Scheck, Kevin and Küster, Dennis and Trancoso, Isabel Schuller, Björn W. and Schultz, Tanja},
    booktitle={{INTERSPEECH} 2020 - 21st Annual Conference of the International Speech Communication Association},
    year={2020 (to appear)},
    url={https://www.csl.uni-bremen.de/cms/images/documents/publications/Diener_IS2020_SilentCompPara.pdf},
    abstract={Silent Computational Paralinguistics (SCP) - the assessment of speaker states and traits from non-audibly spoken communication - has rarely been targeted in the rich body of either Computational Paralinguistics or Silent Speech Processing. Here, we provide first steps towards this challenging but potentially highly rewarding endeavour: Paralinguistics can enrich spoken language interfaces, while Silent Speech Processing enables confidential and unobtrusive spoken communication for everybody, including mute speakers. We approach SCP by using speech-related biosignals stemming from facial muscle activities captured by surface electromyography (EMG). To demonstrate the feasibility of SCP, we select one speaker trait (speaker identity) and one speaker state (speaking mode). We introduce two promising strategies for SCP: (1) deriving paralinguistic speaker information directly from EMG of silently produced speech versus (2) first converting EMG into an audible speech signal followed by conventional computational paralinguistic methods. We compare traditional feature extraction and decision making approaches to more recent deep representation and transfer learning by convolutional and recurrent neural networks, using openly available EMG data. We find that paralinguistics can be assessed not only from acoustic speech but also from silent speech captured by EMG.},
}

@phdthesis{maier-hein2005speech,
  school={Karlsruher Institut für Technologie},
  year={2005},
  title={Speech Recognition using Surface Electromyography},
  supervisor={Schultz, Tanja},
  url={https://www.csl.uni-bremen.de/cms/images/documents/publications/DA-MaierHein.pdf},
  author={Maier-Hein, Lena}
}

@inproceedings{inproceedings,
author = {Wand, Michael and Schultz, Tanja},
year = {2011},
month = {01},
pages = {295-300},
title = {Session-independent EMG-based Speech Recognition.},
journal = {BIOSIGNALS 2011 - Proceedings of the International Conference on Bio-Inspired Systems and Signal Processing}
}
\\
@inproceedings{wand2013artifact,
  year={2013},
  title={Artifact Removal Algorithm for an EMG-based Silent Speech Interface},
  note={EMBC 2013},
  booktitle={International Conference of the IEEE Engineering in Medicine and Biology Society, Osaka, Japan},
  url={https://www.csl.uni-bremen.de/cms/images/documents/publications/WandSchultz_EMBC2013.pdf},
  abstract={An electromygraphic (EMG) Silent Speech Interface is a system which recognizes speech by capturing the electric potentials of the human articulatory muscles, thus enabling the user to communicate silently. This study deals with improving the EMG signal quality by removing artifacts: The EMG signals are captured by electrode arrays with multiple measuring points. On the resulting high-dimensional signal, Independent Component Analysis is performed, and artifact components are automatically detected and removed. This method reduces the Word Error Rate of the silent speech recognizer by 9.9% relative on a development corpus, and by 13.9% relative on an evaluation corpus.},
  author={Wand, Michael and Himmelsbach, Adam and Heistermann, Till and Janke, Matthias and Schultz, Tanja}
}

@inproceedings{janke2010spectral,
  note={Side event of Biosignals 2010 conference},
  year={2010},
  title={Spectral Energy Mapping for EMG-based Recognition of Silent Speech},
  booktitle={First International Workshop on Bio-inspired Human-Machine Interfaces and Healthcare Applications},
  url={https://www.csl.uni-bremen.de/cms/images/documents/publications/WandSchultz_BInterface2010.pdf},
  abstract={This paper reports on our latest study on speech recognition based on surface electromyography (EMG). This technology allows for Silent Speech Interfaces since EMG captures the electrical potentials of the human articulatory muscles rather than the acoustic speech signal. Therefore, our technology enables speech recognition to be applied to silently mouthed speech. Earlier experiments indicate that the EMG signal is greatly impacted by the mode of speaking. In this study we analyze and compare EMG signals from audible, whispered, and silent speech. We quantify the differences and develop a spectral mapping method to compensate for these differences. Finally, we apply the spectral mapping to the front-end of our speech recognition system and show that recognition rates on silent speech improve by up to 12.3% relative.},
  author={Janke, Matthias and Wand, Michael and Schultz, Tanja}
}