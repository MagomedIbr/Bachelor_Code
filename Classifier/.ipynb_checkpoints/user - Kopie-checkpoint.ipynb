{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Magomed\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7, 8], dtype=int64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "\n",
    "import os \n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "import csv\n",
    "import random\n",
    "from sklearn import tree\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from scipy.fftpack import fftn, ifftn, fft, ifft\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import signal\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') \n",
    "df = pd.read_csv(\"Features_all_no_audible.csv\")\n",
    "a = np.unique(df.userID)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_per_session(session_id):\n",
    "    temp_df = df[(df.sessionID == session_id)]\n",
    "    data = temp_df.copy()\n",
    "    y = data['sessionID']\n",
    "    data.drop(labels=['userID','uttID', 'sessionID','modeID'], axis=1, inplace=True)\n",
    "    return train_test_split(data, y, test_size=0.25)\n",
    "\n",
    "def split_data():\n",
    "    temp_df = df\n",
    "    data = temp_df.copy()\n",
    "    y = data['sessionID']\n",
    "    data.drop(labels=['userID','uttID', 'sessionID','modeID'], axis=1, inplace=True)\n",
    "    return train_test_split(data, y, test_size=0.25)\n",
    "\n",
    "def split_data():\n",
    "    temp_df = df\n",
    "    data = temp_df.copy()\n",
    "    y = data['sessionID']\n",
    "    data.drop(labels=['userID','uttID', 'sessionID','modeID'], axis=1, inplace=True)\n",
    "    return train_test_split(data, y, test_size=0.25)\n",
    "\n",
    "def split_data_per_user(user_id):\n",
    "    temp_df = df[(df.userID == user_id)]\n",
    "    sessionIDs = np.unique(temp_df.sessionID)\n",
    "    x1 = pd.DataFrame()\n",
    "    x2 = pd.DataFrame()\n",
    "    y1 = pd.DataFrame()\n",
    "    y2 = pd.DataFrame()\n",
    "    for sessionID in sessionIDs:\n",
    "        temp_dfs = temp_df[(temp_df.sessionID == sessionID)]\n",
    "        data = temp_dfs.copy()\n",
    "        y = data['userID']\n",
    "        data.drop(labels=['userID','uttID', 'sessionID','modeID'], axis=1, inplace=True)\n",
    "        w,x,y,z = train_test_split(data, y, test_size=0.2)\n",
    "        x1 = x1.append(w)\n",
    "        x2 = x2.append(x)\n",
    "        #print(y,y1)\n",
    "        y1 = pd.concat([y1,y])\n",
    "        y2 = pd.concat([y2,z])\n",
    "    return x1,x2,y1,y2\n",
    "\n",
    "def split_data2():\n",
    "    temp_df = df\n",
    "    userIDs = np.unique(df.userID)\n",
    "    x1 = pd.DataFrame()\n",
    "    x2 = pd.DataFrame()\n",
    "    y1 = pd.DataFrame()\n",
    "    y2 = pd.DataFrame()\n",
    "    for userID in userIDs:\n",
    "        temp_dfs = temp_df[(temp_df.userID == userID)]\n",
    "        data = temp_dfs.copy()\n",
    "        y = data['userID']\n",
    "        data.drop(labels=['userID','uttID', 'sessionID','modeID'], axis=1, inplace=True)\n",
    "        w,x,y,z = train_test_split(data, y, test_size=0.25)\n",
    "        x1 = x1.append(w)\n",
    "        x2 = x2.append(x)\n",
    "        #print(y,y1)\n",
    "        y1 = pd.concat([y1,y])\n",
    "        y2 = pd.concat([y2,z])\n",
    "    return x1,x2,y1,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_all_users_separately():\n",
    "    user_ids = np.unique(df.userID)\n",
    "    for user_id in user_ids:\n",
    "        X_train, X_test, y_train, y_test = split_data_per_user(user_id)\n",
    "        clf = LinearDiscriminantAnalysis()\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print(\"Accuracy for user \" + str(user_id) + \" is \" + str(clf.score(X_test, y_test)))\n",
    "\n",
    "def lda_all():\n",
    "    X_train, X_test, y_train, y_test = split_data2()\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Accuracy for users is \"+str(clf.score(X_test, y_test)))\n",
    "    \n",
    "def lda_all_sessions_separately():\n",
    "    sessionIDs = np.unique(df.sessionID)\n",
    "    for sessionID in sessionIDs:\n",
    "        X_train, X_test, y_train, y_test = split_data_per_session(sessionID)\n",
    "        print((y_train))\n",
    "        clf = LinearDiscriminantAnalysis()\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print(\"Accuracy for session \" + str(sessionID) + \" is \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for users is 0.8146300914380715\n"
     ]
    }
   ],
   "source": [
    "lda_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die Session mit einem Score von 1.0 \n",
    "#sind Sessions die jediglich 'Audible' audios enthalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
